{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1-Neural Network from Scratch:"
      ],
      "metadata": {
        "id": "VQQHW68oymN0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oR5d2x37GQG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define sigmoid and its derivative\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -500, 500)  # Prevent overflow\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Initialize weights with Xavier initialization for better convergence\n",
        "def initialize_weights(input_size, output_size):\n",
        "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
        "\n",
        "# Build a simple feed-forward neural network\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights_input_hidden = initialize_weights(input_size, hidden_size)\n",
        "        self.bias_hidden = np.zeros(hidden_size)\n",
        "        self.weights_hidden_output = initialize_weights(hidden_size, output_size)\n",
        "        self.bias_output = np.zeros(output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Input to hidden\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "\n",
        "        # Hidden to output\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output = sigmoid(self.output_input)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        # Calculate output error\n",
        "        output_error = y - self.output\n",
        "        output_delta = output_error * sigmoid_derivative(self.output)\n",
        "\n",
        "        # Calculate hidden layer error\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0) * learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.forward(X)\n",
        "            # Backward pass and update weights\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            # Compute loss (optional)\n",
        "            loss = np.mean(np.square(y - self.output))\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build with PyTorch or TensorFlow:"
      ],
      "metadata": {
        "id": "iyqrNggdytNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess MNIST data\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X = mnist.data / 255.0  # Normalize pixel values to [0, 1]\n",
        "y = mnist.target.astype(int).values\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train.values)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test.values)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "# Define the neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the network, optimizer, and loss function\n",
        "model = NeuralNetwork()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training the network\n",
        "def train(model, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train_tensor)\n",
        "        loss = model.loss_fn(outputs, torch.max(y_train_tensor, 1)[1])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            # Evaluate on test set\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                test_outputs = model(X_test_tensor)\n",
        "                _, predicted = torch.max(test_outputs, 1)\n",
        "                test_labels = torch.max(y_test_tensor, 1)[1]\n",
        "                accuracy = torch.sum(predicted == test_labels).item() / y_test_tensor.size(0)\n",
        "                print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Run training\n",
        "train(model, optimizer)\n"
      ],
      "metadata": {
        "id": "VCoubzGWGVmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff89526-f058-401b-d019-b7ecbce80efd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 2.3566, Test Accuracy: 0.3766\n",
            "Epoch 2/10, Loss: 2.1650, Test Accuracy: 0.4261\n",
            "Epoch 3/10, Loss: 2.0311, Test Accuracy: 0.5309\n",
            "Epoch 4/10, Loss: 1.8946, Test Accuracy: 0.6154\n",
            "Epoch 5/10, Loss: 1.7600, Test Accuracy: 0.6609\n",
            "Epoch 6/10, Loss: 1.6357, Test Accuracy: 0.6797\n",
            "Epoch 7/10, Loss: 1.5220, Test Accuracy: 0.7046\n",
            "Epoch 8/10, Loss: 1.4150, Test Accuracy: 0.7366\n",
            "Epoch 9/10, Loss: 1.3123, Test Accuracy: 0.7649\n",
            "Epoch 10/10, Loss: 1.2149, Test Accuracy: 0.7841\n"
          ]
        }
      ]
    }
  ]
}