# MIA-phase2
1. Ease of Use
From Scratch:

Complexity: Building a neural network from scratch means you have to write all the code for training and updating the model yourself. This can be complicated and time-consuming.
Learning Curve: It requires a deep understanding of how neural networks work. You’ll spend a lot of time figuring out details like how to adjust weights and biases.
PyTorch/TensorFlow:

User-Friendly: These libraries come with ready-made tools and functions to build and train neural networks. You just need to set up your model and let the library handle the rest.
Quick Setup: They simplify many tasks, so you can get your model running faster and with less code.
2. Performance
From Scratch:

Slower: Manual implementations might not be as fast or efficient, especially with larger models or datasets.
Limited Features: You may miss out on advanced techniques that can speed up training and improve performance.
PyTorch/TensorFlow:

Faster: These libraries are optimized for performance and can use GPUs to speed up training, making them much faster for big tasks.
Efficient: They include many built-in optimizations to make training smoother and quicker.
3. Flexibility
From Scratch:

Full Control: You have complete freedom to customize your model exactly as you want. This is great for learning or if you need something very specific.
Educational: Building from scratch helps you understand the inner workings of neural networks.
PyTorch/TensorFlow:

Ease of Experimentation: You can easily try different model types, add layers, or change settings with just a few lines of code.
Tools and Support: They come with many helpful tools for visualizing and debugging your model, which makes it easier to work with.
Scalability: Libraries are designed to handle large-scale training and large datasets. They provide advanced features for distributed training and model parallelism.
3. Flexibility
From Scratch:

Customization: Provides complete control over the network’s architecture and training process. Custom layers, loss functions, and optimizers can be implemented as needed.
Learning Opportunity: Building from scratch allows for a deeper understanding of neural networks' internal workings and can be useful for educational purposes or specific custom implementations.
PyTorch/TensorFlow:

Extensibility: Both libraries are highly flexible and allow for easy experimentation with various architectures, layers, and hyperparameters. They provide extensive APIs and pre-built modules for rapid prototyping.
Integration: Both libraries offer robust ecosystems with tools for model deployment, visualization, and debugging. TensorFlow provides TensorBoard, while PyTorch integrates well with tools like TensorBoardX and Visdo
